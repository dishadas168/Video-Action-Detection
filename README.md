# Video-Action-Detection
Recognition of action being performed in a video segment by computing the Motion History Image of a sequence of images (from a video). We are given 20 sequences performing 5 actions (4 sequences for each action). Given a single sequence, the program predicts which action is being performed. This is performed by calculating Hu Moments for each of the 20 MHIs and calculating the distance between the Hu moments of test case and all the other cases. The sequence having Hu moment with the least distance is the action predicted.

# Credits
See the paper The Representation and Recognition of Action Using Temporal Templates by J. Davis and
A. Bobick for more background on computing the MHI. For additional background on the
properties of Hu moments, see Visual Pattern Recognition by Moment Invariants by M. K. Hu.

Download the Action sequence files from the link below:
https://drive.google.com/open?id=1JYzgujP5bdChHwSgM7VQ8fbW1m3He40v

There are 5 directories, each of which contains 4 sequences for one of the action categories.
The 5 action categories are: botharms, crouch, leftarmup, punch, righkick. Each directory under any one
of these 5 main directories contains the frames for a single sequence. For example, punch/punch-p1-1/
contains one sequence of frames for punch. The data are stored as .pgm images. Each pgm is a grayscale image, where the intensity is proportional to depth in the scene.

# Approach
Download the action folders into the same directory as the executable files given above.
The scripts are executed in the given order:

generateAllMHIs.m - Generates allMHIs.mat file of all MHIs of the 20 sequences. Uses the function computeMHI.m
calculateHuMoments.m - Generates 7-D Hu moment vectors for each of the 20 MHIs and stores them in huVectors.mat file. Uses the function huMoments.m
showNearestMHIs.m - This script displays the top 4 most matched action labels to two sequences of choice. Uses function normDist.m to calculate distances between test case and training cases.
classifyAllActions.m - This script computes the best match for all the 20 actions, taking one at a time as the test case and the other 19 as training cases. It then computes the Confusion matrix, the overall recognition rate and recognition rates for each action. Uses function predictAction.m to predict the action label.

# Examples
The MHI for a sequence in botharms action category is given below:

The output generated by showNearestMHIs.m is given below. The input action is a botharms action. The output are the 4 best-matched actions:

Confusion matrix and recognition rates computed after executing classifyAllActions.m







